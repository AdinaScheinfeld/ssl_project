# config file for lsm pretraining

data:
  data_dir: /midtier/paetzollab/scratch/ads4015/all_wu_brain_patches
  train_frac: 0.8 # fraction of volumes to use for training (remaining used for val)
  batch_size: 2 # number of patches per batch

model:
  image_size: 96
  mask_ratio: 0.3 # fraction of masked patches per volume
  lr: 0.0001
  ema_decay: 0.996
  mask_patch_size: 16 # base patch size for masking
  temp_student: 0.1 # temperature for student logits
  temp_teacher: 0.04 # temperature for teacher softmax (0.04 is default in most iBOT implementations)
  text_model_name: bert-base-uncased # text model to use (bert-base-uncased from HuggingFace)
  embed_dim: 256 # 256 is balance of GPU memory and representational power (larger values may have better performance but also higher compute/memory cost)
  clip_temperature: 0.07 # (0.07 is default in OpenAI paper and used by most applications)
  save_filename: wu_clip_pretrained_1
  save_dirpath: /home/ads4015/ssl_project/checkpoints
  feature_size: 48
  max_log_images: 5

loss_weights:
  distill_weight: 0.5 # masked voxel KL divergence
  reconstruction_weight: 0.4 # masked voxel L1 reconstructions
  align_weight: 0.1 # image-text cosine similarity per pair
  clip_weight: 1.0 # global contrastive loss between all image and text embeddings in batch

training:
  seed: 100
  max_epochs: 1000
  patience: 50
  log_every_n_steps: 5
  project_name: ibot-clip-pretrain-lsm
















