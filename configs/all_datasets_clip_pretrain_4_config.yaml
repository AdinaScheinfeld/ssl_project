# all_datasets_clip_pretrain_2_config.yaml - config file for lsm pretraining

training:
  seed: 100
  project_name: ibot-clip-pretrain-lsm-all
  max_epochs: 1000
  patience: 45
  log_every_n_steps: 5

# config for distributed training
dist:
  multi_gpu: True # set to True for multi-gpu training
  accelerator: gpu # cpu/gpu/mps
  strategy: ddp # distributed data parallel (ddp) for multi-gpu training 
  # (one process per gpu where each process has its own copy of the model and optimiser; 
  # training data is split across processes and sees a unique mini-batch each step; 
  # after each forward/backward pass, gradients are averaged acrosss all gpus so models stay in sync)
  devices: 4 # number of gpus per node (set to 1 for single gpu training)
  num_nodes: 1 # set >1 for multi-node training
  precision: bf16-mixed # bf16-mixed good for h100 gpus (some parts of computation use bfloat16 for speed and memory efficiency, while others use float32 for stability)
  accumulate_grad_batches: 2 # gradient accumulation (controls how many batches to accumulate gradients over before performing an optimizer step; set to >1 for larger effective batch size)
  sync_batchnorm: True # synchronize batch norm across gpus (good for multi-gpu training)
  deterministic: False # set to True for reproducibility (may slow down training)
  
data:

  # paths to all datasets
  roots:
    connection: /midtier/paetzollab/scratch/ads4015/all_allen_connection_projection_patches
    dev_mouse: /midtier/paetzollab/scratch/ads4015/all_allen_developing_mouse_patches
    human2: /midtier/paetzollab/scratch/ads4015/all_allen_human2_patches
    selma: /midtier/paetzollab/scratch/ads4015/all_selma_patches_96
    wu: /midtier/paetzollab/scratch/ads4015/all_wu_brain_patches

  enable:
    connection: True
    dev_mouse: True
    human2: True
    selma: True
    wu: True

  prompt_jsons:
    - /home/ads4015/ssl_project/data/text_prompts_allen_connection.json
    - /home/ads4015/ssl_project/data/text_prompts_allen_dev_mouse.json
    - /home/ads4015/ssl_project/data/text_prompts_allen_human2.json
    - /home/ads4015/ssl_project/data/text_prompts_selma.json
    - /home/ads4015/ssl_project/data/text_prompts_wu.json

  # batching
  global_batch_size: 32 # total batch size across all gpus (ex: use 32 for 4 gpus with batch_size=8)
  batch_size: 16 #32 # number of patches per batch (use max 8 for 96x96x96 patches on 96GB GPU h100 GPUs) ***

  # split and sampling
  train_frac: 0.9 # fraction of volumes to use for training (remaining used for val)
  data_subset_frac: 1.0 #0.5 # percent of files to use per volume (set to 1.0 to use all files) ***
  per_source_frac: {}
  per_source_max: {}

  # patch sizing
  use_sub_patches: False #True # if true, will divide all each 96x96x96 patch into 2 sub_patches of size 64x64x64 ***
  base_patch_size: 96 # size of each patch in voxels (use: 96x96x96)
  sub_patch_size: 64 # size of each sub_patch in voxels (use: 64x64x64) if use_sub_patches is True

  # loader
  num_workers: 4 # number of workers for data loading

  # downsampling
  downsample:
    enabled: True
    target_size: 64 # resample 96^3 -> 64^3 (lower resolution)


model:
  mask_ratio_warmup: 0.1 # fraction of masked patches per volume for first few epochs
  warmup_epochs: 20
  mask_ratio: 0.3 # fraction of masked patches per volume
  lr: 0.0001
  ema_decay: 0.996
  mask_patch_size: 8 # base patch size for masking (use: 16x16x16 for 96x96x96 base patches, or 8x8x8 for 64x64x64 sub_patches) ***
  temp_student: 0.1 # temperature for student logits
  temp_teacher: 0.04 # temperature for teacher softmax (0.04 is default in most iBOT implementations)
  text_model_name: bert-base-uncased # text model to use (bert-base-uncased from HuggingFace)
  embed_dim: 256 # 256 is balance of GPU memory and representational power (larger values may have better performance but also higher compute/memory cost)
  clip_temperature: 0.07 # (0.07 is default in OpenAI paper and used by most applications)
  save_filename: all_datasets_clip_pretrained
  save_dirpath: /home/ads4015/ssl_project/checkpoints
  feature_size: 24 # 48
  max_log_images: 5
  finetune_text: True # if True, will finetune text encoder during pretraining (may improve alignment but increases memory usage and training time)
  text_top_k_layers: 4 # number of top layers of text transformer to train (only used if finetune_text is True
  text_finetune_start_epoch: 20 # epoch to start finetuning text encoder (only used if finetune_text is True


loss_weights:
  distill_weight: 0.6 # masked voxel KL divergence
  reconstruction_weight: 0.3 # masked voxel L1 reconstructions
  align_weight: 0.1 # image-text cosine similarity per pair
  clip_weight: 1.0 # global contrastive loss between all image and text embeddings in batch




















