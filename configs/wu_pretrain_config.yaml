# config file for lsm pretraining

data:
  data_dir: "/midtier/paetzollab/scratch/ads4015/all_wu_brain_patches"
  train_frac: 0.8 # fraction of volumes to use for training (remaining used for val)
  batch_size: 8 # number of patches per batch

model:
  image_size: 96
  mask_ratio: 0.3 # fraction of masked patches per volume
  lr: 0.0001
  ema_decay: 0.996
  mask_patch_size: 16 # base patch size for masking
  temp_student: 0.1 # temperature for student logits
  temp_teacher: 0.04 # temperature for teacher softmax

training:
  seed: 100
  max_epochs: 1000
  patience: 10
  log_every_n_steps: 5
  project_name: 'ibot-pretrain-lsm'
















