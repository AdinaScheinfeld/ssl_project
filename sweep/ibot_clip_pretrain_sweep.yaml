# sweep/ibot_clip_pretrain_sweep.yaml
# TUNES: max_epochs, patience, accumulate_grad_batches, global_batch_size,
#        num_workers, downsample.enabled, mask_ratio_warmup, warmup_epochs,
#        ema_decay, mask_patch_size, temp_student, temp_teacher,
#        embed_dim, clip_temperature, feature_size
# Keeps: seed=100, devices=2, train_frac=0.9, data_subset_frac=1.0,
#        use_sub_patches=False, base_patch_size=96, target_size=64

program: sweep/run_pretrain_wandb.py # small wrapper that merges overrides and launches trainer
name: ibot-clip-pretrain-lsm-sweep
project: ibot-clip-pretrain-lsm-all # matches training.project_name in your base YAML

method: bayes
metric:
  name: val_loss
  goal: minimize

parameters:
  training.max_epochs:
    values: [500, 1000, 2000]
  training.patience:
    values: [100, 150, 200]

  dist.accumulate_grad_batches:
    values: [4, 8, 16]

  data.global_batch_size:
    values: [8, 16, 32]  # perâ€‘GPU BS = global / (2 * accumulate)
  data.num_workers:
    values: [0, 1]
  data.downsample.enabled:
    values: [true, false]

  model.mask_ratio_warmup:
    values: [0.05, 0.1]
  model.warmup_epochs:
    values: [20, 30, 50]
  model.ema_decay:
    values: [0.996, 0.998]
  model.mask_patch_size:
    values: [8]
  model.temp_student:
    values: [0.025, 0.05, 0.1]
  model.temp_teacher:
    values: [0.05]
  model.embed_dim:
    values: [512]
  model.clip_temperature:
    values: [0.04, 0.07, 0.1]
  model.feature_size:
    values: [6, 12, 24]

# prune weak trials early to recycle GPUs
early_terminate:
  type: hyperband
  min_iter: 15



