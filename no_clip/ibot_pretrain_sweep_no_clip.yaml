# no_clip/ibot_pretrain_sweep_no_clip.yaml
# TUNES: max_epochs, patience, accumulate_grad_batches, global_batch_size,
#        num_workers, downsample.enabled, mask_ratio_warmup, warmup_epochs,
#        ema_decay, mask_patch_size, temp_student, temp_teacher,
#        embed_dim, clip_temperature, feature_size
# Keeps: seed=100, devices=2, train_frac=0.9, data_subset_frac=1.0,
#        use_sub_patches=False, base_patch_size=96, target_size=64

program: no_clip/run_pretrain_wandb_no_clip.py # small wrapper that merges overrides and launches trainer
name: ibot-pretrain-lsm-sweep-no-clip
project: ibot-no-clip-pretrain-lsm-all # matches training.project_name in your base YAML

method: bayes
metric:
  name: val_loss
  goal: minimize

parameters:
  training.max_epochs:
    values: [200, 500, 1000]
  training.patience:
    values: [20, 50, 100]

  dist.accumulate_grad_batches:
    values: [1, 2, 4]

  data.global_batch_size:
    values: [16, 32, 64]  # perâ€‘GPU BS = global / (2 * accumulate)
  data.num_workers:
    values: [0, 1, 2]
  data.downsample.enabled:
    values: [true, false]

  model.mask_ratio_warmup:
    values: [0.05, 0.1, 0.2]
  model.warmup_epochs:
    values: [10, 20, 30]
  model.ema_decay:
    values: [0.994, 0.996, 0.998]
  model.mask_patch_size:
    values: [4, 8, 16]
  model.temp_student:
    values: [0.05, 0.1, 0.2]
  model.temp_teacher:
    values: [0.03, 0.04, 0.05]
  model.embed_dim:
    values: [128, 256, 512]
  model.feature_size:
    values: [12, 24, 48]
  model.lr:
    values: [0.00005, 0.0001, 0.0002]
  model.mask_ratio:
    values: [0.2, 0.3, 0.4]

# prune weak trials early to recycle GPUs
early_terminate:
  type: hyperband
  min_iter: 15



