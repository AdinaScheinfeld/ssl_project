{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "463a0c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ads4015/micromamba/envs/micro-sam-gpu/lib/python3.10/site-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT exists: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from skimage.measure import label as connected_components\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import scipy.ndimage as ndi\n",
    "\n",
    "import micro_sam.training as sam_training\n",
    "from micro_sam.automatic_segmentation import (\n",
    "    get_predictor_and_segmenter,\n",
    "    automatic_instance_segmentation,\n",
    ")\n",
    "\n",
    "# Root directory with your finetune patches\n",
    "ROOT = Path(\"/midtier/paetzollab/scratch/ads4015/data_selma3d/selma3d_finetune_patches\")\n",
    "print(\"ROOT exists:\", ROOT.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fe3ca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance_to_center(inst_mask: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    For each instance ID > 0, compute distance to its center of mass.\n",
    "    Normalize to [0,1] per instance and invert so that center=1, edges=0.\n",
    "    Outside objects: 0.\n",
    "    \"\"\"\n",
    "    H, W = inst_mask.shape\n",
    "    out = np.zeros((H, W), dtype=np.float32)\n",
    "\n",
    "    instance_ids = np.unique(inst_mask)\n",
    "    instance_ids = instance_ids[instance_ids != 0]\n",
    "\n",
    "    yy, xx = np.indices((H, W), dtype=np.float32)\n",
    "\n",
    "    for iid in instance_ids:\n",
    "        mask = (inst_mask == iid)\n",
    "        if not np.any(mask):\n",
    "            continue\n",
    "\n",
    "        cy, cx = ndi.center_of_mass(mask.astype(np.float32))\n",
    "        dist = np.sqrt((yy - cy) ** 2 + (xx - cx) ** 2)\n",
    "\n",
    "        maxd = dist[mask].max() + 1e-6\n",
    "        dist_norm = dist / maxd  # 0..1 inside object\n",
    "        out[mask] = 1.0 - dist_norm[mask]  # 1 at center, 0 at furthest\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def compute_distance_to_boundary(inst_mask: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    For each instance ID > 0, compute distance transform inside the object.\n",
    "    Normalize to [0,1] per instance. Outside objects: 0.\n",
    "    \"\"\"\n",
    "    H, W = inst_mask.shape\n",
    "    out = np.zeros((H, W), dtype=np.float32)\n",
    "\n",
    "    instance_ids = np.unique(inst_mask)\n",
    "    instance_ids = instance_ids[instance_ids != 0]\n",
    "\n",
    "    for iid in instance_ids:\n",
    "        mask = (inst_mask == iid)\n",
    "        if not np.any(mask):\n",
    "            continue\n",
    "\n",
    "        dist = ndi.distance_transform_edt(mask)\n",
    "        maxd = dist[mask].max() + 1e-6\n",
    "        dist_norm = dist / maxd  # 0..1 inside object\n",
    "        out[mask] = dist_norm[mask]\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24a28c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Selma2DSliceDataset(Dataset):\n",
    "    def __init__(self, raw_label_pairs):\n",
    "        \"\"\"\n",
    "        raw_label_pairs: list of (raw_path, label_path)\n",
    "        \"\"\"\n",
    "        self.pairs = raw_label_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        raw_path, label_path = self.pairs[idx]\n",
    "\n",
    "        # Load volumes\n",
    "        raw_vol = nib.load(str(raw_path)).get_fdata().astype(np.float32)\n",
    "        lab_vol = nib.load(str(label_path)).get_fdata().astype(np.float32)\n",
    "\n",
    "        assert raw_vol.shape == lab_vol.shape, f\"shape mismatch: {raw_vol.shape} vs {lab_vol.shape}\"\n",
    "        Z = raw_vol.shape[0]\n",
    "\n",
    "        # --- find a slice that contains at least one object ---\n",
    "        for _ in range(20):\n",
    "            z = np.random.randint(0, Z)\n",
    "            lab_slice = lab_vol[z]\n",
    "            if lab_slice.max() > 0:\n",
    "                break\n",
    "        else:\n",
    "            # fallback: deterministic scan\n",
    "            found = False\n",
    "            for z in range(Z):\n",
    "                lab_slice = lab_vol[z]\n",
    "                if lab_slice.max() > 0:\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                # no instances in this volume at all; pick a different pair\n",
    "                return self.__getitem__((idx + 1) % len(self.pairs))\n",
    "\n",
    "        img_slice = raw_vol[z]\n",
    "        lab_slice = lab_vol[z]\n",
    "\n",
    "        # --- instance mask (connected components) ---\n",
    "        bin_mask = (lab_slice > 0).astype(np.uint8)\n",
    "        inst_mask = connected_components(bin_mask, connectivity=1).astype(np.int32)\n",
    "\n",
    "        if inst_mask.max() == 0:\n",
    "            # no instance after CC, rare corner case\n",
    "            return self.__getitem__((idx + 1) % len(self.pairs))\n",
    "\n",
    "        # --- robust image normalization to [0,255] for SAM ---\n",
    "        p1 = np.percentile(img_slice, 1)\n",
    "        p99 = np.percentile(img_slice, 99)\n",
    "        if p99 > p1:\n",
    "            img_slice_norm = (img_slice - p1) / (p99 - p1)\n",
    "        else:\n",
    "            img_slice_norm = np.zeros_like(img_slice, dtype=np.float32)\n",
    "\n",
    "        img_slice_norm = np.clip(img_slice_norm, 0.0, 1.0)\n",
    "        img_slice_uint = (img_slice_norm * 255.0).astype(np.float32)\n",
    "\n",
    "        # --- label channels ---\n",
    "        # channel 0: instance ids\n",
    "        instance_channel = inst_mask.astype(np.float32)\n",
    "\n",
    "        # foreground mask\n",
    "        fg = (inst_mask > 0).astype(np.float32)\n",
    "\n",
    "        # distances\n",
    "        dist_center = compute_distance_to_center(inst_mask)\n",
    "        dist_boundary = compute_distance_to_boundary(inst_mask)\n",
    "\n",
    "        # mask distances and clamp to [0,1]\n",
    "        dist_center = np.clip(dist_center * fg, 0.0, 1.0)\n",
    "        dist_boundary = np.clip(dist_boundary * fg, 0.0, 1.0)\n",
    "\n",
    "        # assemble y: (4, H, W)\n",
    "        # IMPORTANT: channel order to match microSAM expectations:\n",
    "        #   y[0] = instance ids (integer-valued)\n",
    "        #   y[1] = foreground (0..1)\n",
    "        #   y[2] = center distance (0..1)\n",
    "        #   y[3] = boundary distance (0..1)\n",
    "        y_np = np.stack(\n",
    "            [instance_channel, fg, dist_center, dist_boundary],\n",
    "            axis=0\n",
    "        ).astype(np.float32)\n",
    "\n",
    "        # to torch\n",
    "        x = torch.from_numpy(img_slice_uint[None, ...])  # (1,H,W), float32 in [0,255]\n",
    "        y = torch.from_numpy(y_np)                        # (4,H,W)\n",
    "\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55a9add4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 68 raw/label volumes\n"
     ]
    }
   ],
   "source": [
    "pairs = []\n",
    "\n",
    "for class_dir in ROOT.iterdir():\n",
    "    if not class_dir.is_dir():\n",
    "        continue\n",
    "\n",
    "    # use ch0 as the image (for vessels: patch_*_ch0.nii.gz)\n",
    "    for raw_path in sorted(class_dir.glob(\"*_ch0.nii.gz\")):\n",
    "        label_path = raw_path.with_name(raw_path.name.replace(\".nii.gz\", \"_label.nii.gz\"))\n",
    "        if label_path.exists():\n",
    "            pairs.append((raw_path, label_path))\n",
    "\n",
    "print(\"Found\", len(pairs), \"raw/label volumes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebad6980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 61 Val samples: 7\n"
     ]
    }
   ],
   "source": [
    "# simple random split\n",
    "rng = np.random.default_rng(42)\n",
    "perm = rng.permutation(len(pairs))\n",
    "pairs = [pairs[i] for i in perm]\n",
    "\n",
    "n_train = int(0.9 * len(pairs))\n",
    "train_pairs = pairs[:n_train]\n",
    "val_pairs = pairs[n_train:]\n",
    "\n",
    "train_dataset = Selma2DSliceDataset(train_pairs)\n",
    "val_dataset = Selma2DSliceDataset(val_pairs)\n",
    "\n",
    "batch_size = 1  # microSAM typically uses small batch sizes\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# microSAM's JointSamTrainer expects these attributes\n",
    "train_loader.shuffle = True\n",
    "val_loader.shuffle = False\n",
    "\n",
    "print(\"Train samples:\", len(train_dataset), \"Val samples:\", len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "449e6b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verifying labels in 'train' dataloader: 100%|██████████| 50/50 [00:00<00:00, 54.26it/s]\n",
      "Verifying labels in 'val' dataloader:  14%|█▍        | 7/50 [00:00<00:01, 38.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start fitting for 610 iterations /  10 epochs\n",
      "with 61 iterations per epoch\n",
      "Training with mixed precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: average [s/it]: 0.441431, current metric: 0.431691, best metric: 0.258571: 100%|█████████▉| 609/610 [05:23<00:00,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training after 10 epochs / 610 iterations.\n",
      "The best epoch is number 7.\n",
      "Training took 326.2011938095093 seconds (= 00:05:26 hours)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "n_objects_per_batch = 5\n",
    "train_instance_segmentation = True\n",
    "model_type = \"vit_b\"  # must match when loading later\n",
    "\n",
    "checkpoint_name = \"selma3d_microsam_ais\"\n",
    "root_dir = os.getcwd()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "sam_training.train_sam(\n",
    "    name=checkpoint_name,\n",
    "    save_root=os.path.join(root_dir, \"models\"),\n",
    "    model_type=model_type,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    n_epochs=n_epochs,\n",
    "    n_objects_per_batch=n_objects_per_batch,\n",
    "    with_segmentation_decoder=train_instance_segmentation,\n",
    "    device=device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d57ada95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best checkpoint: /home/ads4015/ssl_project/models/checkpoints/selma3d_microsam_ais/best.pt exists: True\n"
     ]
    }
   ],
   "source": [
    "checkpoint_name = \"selma3d_microsam_ais\"\n",
    "best_checkpoint = os.path.join(\n",
    "    root_dir,\n",
    "    \"models\",\n",
    "    \"checkpoints\",\n",
    "    checkpoint_name,\n",
    "    \"best.pt\",\n",
    ")\n",
    "print(\"Best checkpoint:\", best_checkpoint, \"exists:\", os.path.exists(best_checkpoint))\n",
    "\n",
    "predictor, segmenter = get_predictor_and_segmenter(\n",
    "    model_type=model_type,\n",
    "    checkpoint=best_checkpoint,\n",
    "    device=device,\n",
    "    is_tiled=False,  # we will handle tiling for large images ourselves\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "628ceafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_slice_2d(predictor, segmenter, img2d, tile_shape=None, halo=None, verbose=False):\n",
    "    \"\"\"Run AIS on a single 2D slice.\"\"\"\n",
    "    img2d = img2d.astype(np.float32)\n",
    "\n",
    "    # normalize to [0,255] similarly to training\n",
    "    p1 = np.percentile(img2d, 1)\n",
    "    p99 = np.percentile(img2d, 99)\n",
    "    if p99 > p1:\n",
    "        img2d = (img2d - p1) / (p99 - p1)\n",
    "    else:\n",
    "        img2d = np.zeros_like(img2d)\n",
    "    img2d = np.clip(img2d, 0.0, 1.0)\n",
    "    img2d = (img2d * 255.0).astype(np.float32)\n",
    "\n",
    "    instances = automatic_instance_segmentation(\n",
    "        predictor=predictor,\n",
    "        segmenter=segmenter,\n",
    "        input_path=img2d,\n",
    "        ndim=2,\n",
    "        tile_shape=tile_shape,\n",
    "        halo=halo,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    return instances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "caef9d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_volume_slices(predictor, segmenter, vol, tile_shape=None, halo=None, verbose=False):\n",
    "    \"\"\"\n",
    "    vol: numpy array (Z, Y, X)\n",
    "    returns: instances (Z, Y, X) with per-slice instance labels.\n",
    "    \"\"\"\n",
    "    Z, Y, X = vol.shape\n",
    "    instances_vol = np.zeros((Z, Y, X), dtype=np.int32)\n",
    "\n",
    "    for z in range(Z):\n",
    "        if verbose:\n",
    "            print(f\"Segmenting slice {z+1}/{Z}...\")\n",
    "        img2d = vol[z]\n",
    "        seg2d = segment_slice_2d(\n",
    "            predictor,\n",
    "            segmenter,\n",
    "            img2d,\n",
    "            tile_shape=tile_shape,\n",
    "            halo=halo,\n",
    "            verbose=False,\n",
    "        )\n",
    "        instances_vol[z] = seg2d.astype(np.int32)\n",
    "\n",
    "    return instances_vol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e031af41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting slice 1/96...\n",
      "Segmenting slice 2/96...\n",
      "Segmenting slice 3/96...\n",
      "Segmenting slice 4/96...\n",
      "Segmenting slice 5/96...\n",
      "Segmenting slice 6/96...\n",
      "Segmenting slice 7/96...\n",
      "Segmenting slice 8/96...\n",
      "Segmenting slice 9/96...\n",
      "Segmenting slice 10/96...\n",
      "Segmenting slice 11/96...\n",
      "Segmenting slice 12/96...\n",
      "Segmenting slice 13/96...\n",
      "Segmenting slice 14/96...\n",
      "Segmenting slice 15/96...\n",
      "Segmenting slice 16/96...\n",
      "Segmenting slice 17/96...\n",
      "Segmenting slice 18/96...\n",
      "Segmenting slice 19/96...\n",
      "Segmenting slice 20/96...\n",
      "Segmenting slice 21/96...\n",
      "Segmenting slice 22/96...\n",
      "Segmenting slice 23/96...\n",
      "Segmenting slice 24/96...\n",
      "Segmenting slice 25/96...\n",
      "Segmenting slice 26/96...\n",
      "Segmenting slice 27/96...\n",
      "Segmenting slice 28/96...\n",
      "Segmenting slice 29/96...\n",
      "Segmenting slice 30/96...\n",
      "Segmenting slice 31/96...\n",
      "Segmenting slice 32/96...\n",
      "Segmenting slice 33/96...\n",
      "Segmenting slice 34/96...\n",
      "Segmenting slice 35/96...\n",
      "Segmenting slice 36/96...\n",
      "Segmenting slice 37/96...\n",
      "Segmenting slice 38/96...\n",
      "Segmenting slice 39/96...\n",
      "Segmenting slice 40/96...\n",
      "Segmenting slice 41/96...\n",
      "Segmenting slice 42/96...\n",
      "Segmenting slice 43/96...\n",
      "Segmenting slice 44/96...\n",
      "Segmenting slice 45/96...\n",
      "Segmenting slice 46/96...\n",
      "Segmenting slice 47/96...\n",
      "Segmenting slice 48/96...\n",
      "Segmenting slice 49/96...\n",
      "Segmenting slice 50/96...\n",
      "Segmenting slice 51/96...\n",
      "Segmenting slice 52/96...\n",
      "Segmenting slice 53/96...\n",
      "Segmenting slice 54/96...\n",
      "Segmenting slice 55/96...\n",
      "Segmenting slice 56/96...\n",
      "Segmenting slice 57/96...\n",
      "Segmenting slice 58/96...\n",
      "Segmenting slice 59/96...\n",
      "Segmenting slice 60/96...\n",
      "Segmenting slice 61/96...\n",
      "Segmenting slice 62/96...\n",
      "Segmenting slice 63/96...\n",
      "Segmenting slice 64/96...\n",
      "Segmenting slice 65/96...\n",
      "Segmenting slice 66/96...\n",
      "Segmenting slice 67/96...\n",
      "Segmenting slice 68/96...\n",
      "Segmenting slice 69/96...\n",
      "Segmenting slice 70/96...\n",
      "Segmenting slice 71/96...\n",
      "Segmenting slice 72/96...\n",
      "Segmenting slice 73/96...\n",
      "Segmenting slice 74/96...\n",
      "Segmenting slice 75/96...\n",
      "Segmenting slice 76/96...\n",
      "Segmenting slice 77/96...\n",
      "Segmenting slice 78/96...\n",
      "Segmenting slice 79/96...\n",
      "Segmenting slice 80/96...\n",
      "Segmenting slice 81/96...\n",
      "Segmenting slice 82/96...\n",
      "Segmenting slice 83/96...\n",
      "Segmenting slice 84/96...\n",
      "Segmenting slice 85/96...\n",
      "Segmenting slice 86/96...\n",
      "Segmenting slice 87/96...\n",
      "Segmenting slice 88/96...\n",
      "Segmenting slice 89/96...\n",
      "Segmenting slice 90/96...\n",
      "Segmenting slice 91/96...\n",
      "Segmenting slice 92/96...\n",
      "Segmenting slice 93/96...\n",
      "Segmenting slice 94/96...\n",
      "Segmenting slice 95/96...\n",
      "Segmenting slice 96/96...\n",
      "Instances shape: (96, 96, 96)\n",
      "Unique labels (first 10): [0 1 2 3 4 5 6 7 8 9]\n",
      "Saved: /midtier/paetzollab/scratch/ads4015/data_selma3d/selma3d_finetune_patches/vessels_patches/patch_001_vol021_ch1_microsam_ais2dstack.nii.gz\n"
     ]
    }
   ],
   "source": [
    "test_path = str(\n",
    "    \"/midtier/paetzollab/scratch/ads4015/data_selma3d/selma3d_finetune_patches/vessels_patches/patch_001_vol021_ch1.nii.gz\"\n",
    ")\n",
    "\n",
    "nii = nib.load(test_path)\n",
    "vol = nii.get_fdata().astype(np.float32)  # (Z, Y, X)\n",
    "\n",
    "instances = segment_volume_slices(\n",
    "    predictor,\n",
    "    segmenter,\n",
    "    vol,\n",
    "    tile_shape=None,  # no tiling needed for 96^3 patches\n",
    "    halo=None,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"Instances shape:\", instances.shape)\n",
    "print(\"Unique labels (first 10):\", np.unique(instances)[:10])\n",
    "\n",
    "out_path = test_path.replace(\".nii.gz\", \"_microsam_ais2dstack.nii.gz\")\n",
    "out_nii = nib.Nifti1Image(instances.astype(np.int32), affine=nii.affine, header=nii.header)\n",
    "nib.save(out_nii, out_path)\n",
    "print(\"Saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50cabc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micro-sam-gpu",
   "language": "python",
   "name": "micro-sam-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
